"""è¤‡æ•°ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®æ€§èƒ½æ¯”è¼ƒãƒ‡ãƒ¢"""

import os
import sys
import time
import numpy as np
from typing import Dict, List, Tuple
from stable_baselines3 import DQN, PPO
from stable_baselines3.common.monitor import Monitor
import matplotlib.pyplot as plt

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))
from ml.lib.mega_wing_env import MegaWingEnv


class AgentComparison:
    """è¤‡æ•°ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®æ€§èƒ½æ¯”è¼ƒ"""
    
    def __init__(self, evaluation_episodes: int = 10, max_steps: int = 2000):
        self.evaluation_episodes = evaluation_episodes
        self.max_steps = max_steps
        self.results: Dict[str, Dict[str, float]] = {}
        
    def create_agents(self) -> Dict[str, any]:
        """å„ç¨®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’ä½œæˆ"""
        agents = {}
        
        print("Creating agents...")
        
        # ç’°å¢ƒä½œæˆï¼ˆè¨“ç·´ç”¨ï¼‰
        train_env = MegaWingEnv(observation_type="vector", max_steps=1000)
        train_env = Monitor(train_env)
        
        # DQNã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆï¼ˆã‚¯ã‚¤ãƒƒã‚¯è¨“ç·´ï¼‰
        print("Training DQN agent...")
        dqn_agent = DQN(
            policy='MlpPolicy',
            env=train_env,
            learning_rate=1e-3,
            buffer_size=20000,
            learning_starts=500,
            batch_size=32,
            train_freq=4,
            target_update_interval=500,
            exploration_fraction=0.4,
            exploration_final_eps=0.05,
            verbose=0
        )
        dqn_agent.learn(total_timesteps=8000, progress_bar=True)
        agents['DQN'] = dqn_agent
        
        # PPOã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆï¼ˆã‚¯ã‚¤ãƒƒã‚¯è¨“ç·´ï¼‰
        print("Training PPO agent...")
        ppo_agent = PPO(
            policy='MlpPolicy',
            env=train_env,
            learning_rate=3e-4,
            n_steps=512,
            batch_size=32,
            n_epochs=8,
            gamma=0.99,
            gae_lambda=0.95,
            clip_range=0.2,
            ent_coef=0.01,
            verbose=0
        )
        ppo_agent.learn(total_timesteps=8000, progress_bar=True)
        agents['PPO'] = ppo_agent
        
        # ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆï¼ˆé–¢æ•°ã¨ã—ã¦å®Ÿè£…ï¼‰
        def rule_based_agent(obs):
            """ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ"""
            player_x = obs[0] * 120
            can_shoot = obs[2] > 0.5
            
            # æ•µæƒ…å ±ï¼ˆæœ€ã‚‚è¿‘ã„2ä½“ï¼‰
            enemy1_x = obs[6] * 120 if obs[6] > 0 else None
            enemy1_y = obs[7] * 160 if obs[7] > 0 else None
            
            # åŸºæœ¬æˆ¦ç•¥
            center_x = 60
            
            # ä¸­å¤®ç¶­æŒ + å°„æ’ƒ
            if player_x < center_x - 15:
                return MegaWingEnv.ACTION_RIGHT
            elif player_x > center_x + 15:
                return MegaWingEnv.ACTION_LEFT
            elif can_shoot and enemy1_x is not None and abs(enemy1_x - player_x) < 30:
                return MegaWingEnv.ACTION_SHOOT
            else:
                return MegaWingEnv.ACTION_NO_OP
        
        agents['Rule-based'] = rule_based_agent
        
        # ãƒ©ãƒ³ãƒ€ãƒ ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ
        def random_agent(obs):
            """ãƒ©ãƒ³ãƒ€ãƒ ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ"""
            actions = [0, 1, 2, 3, 4, 5]  # NO_OP, LEFT, RIGHT, UP, DOWN, SHOOT
            # ä½•ã‚‚ã—ãªã„ç¢ºç‡ã‚’é«˜ã‚ã‚‹
            weights = [0.4, 0.1, 0.1, 0.1, 0.1, 0.2]
            return np.random.choice(actions, p=weights)
        
        agents['Random'] = random_agent
        
        print(f"Created {len(agents)} agents")
        return agents
    
    def evaluate_agent(self, agent_name: str, agent, env: MegaWingEnv) -> Dict[str, float]:
        """å˜ä¸€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®è©•ä¾¡"""
        print(f"Evaluating {agent_name}...")
        
        episode_rewards = []
        episode_scores = []
        episode_lengths = []
        survived_episodes = 0
        
        for episode in range(self.evaluation_episodes):
            obs, _ = env.reset()
            episode_reward = 0
            episode_length = 0
            
            while True:
                if agent_name in ['DQN', 'PPO']:
                    action, _ = agent.predict(obs, deterministic=True)
                else:
                    # é–¢æ•°å‹ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ
                    action = agent(obs)
                
                obs, reward, done, truncated, info = env.step(action)
                episode_reward += reward
                episode_length += 1
                
                if done or truncated:
                    break
            
            episode_rewards.append(episode_reward)
            episode_scores.append(info.get('score', 0))
            episode_lengths.append(episode_length)
            
            if episode_length >= self.max_steps * 0.8:  # 80%ä»¥ä¸Šç”Ÿå­˜
                survived_episodes += 1
        
        # çµ±è¨ˆè¨ˆç®—
        results = {
            'avg_reward': np.mean(episode_rewards),
            'std_reward': np.std(episode_rewards),
            'max_reward': np.max(episode_rewards),
            'min_reward': np.min(episode_rewards),
            'avg_score': np.mean(episode_scores),
            'max_score': np.max(episode_scores),
            'avg_survival': np.mean(episode_lengths),
            'survival_rate': survived_episodes / self.evaluation_episodes,
            'episodes': self.evaluation_episodes
        }
        
        print(f"  Avg Reward: {results['avg_reward']:.2f} Â± {results['std_reward']:.2f}")
        print(f"  Max Score: {results['max_score']}")
        print(f"  Survival Rate: {results['survival_rate']:.1%}")
        
        return results
    
    def run_comparison(self) -> Dict[str, Dict[str, float]]:
        """å…¨ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®æ¯”è¼ƒå®Ÿè¡Œ"""
        print("=== Agent Performance Comparison ===")
        print(f"Episodes per agent: {self.evaluation_episodes}")
        print(f"Max steps per episode: {self.max_steps}")
        print()
        
        # ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆä½œæˆ
        agents = self.create_agents()
        
        # è©•ä¾¡ç”¨ç’°å¢ƒ
        eval_env = MegaWingEnv(observation_type="vector", max_steps=self.max_steps)
        
        # å„ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’è©•ä¾¡
        for agent_name, agent in agents.items():
            self.results[agent_name] = self.evaluate_agent(agent_name, agent, eval_env)
            print()
        
        return self.results
    
    def print_comparison_table(self):
        """æ¯”è¼ƒçµæœãƒ†ãƒ¼ãƒ–ãƒ«è¡¨ç¤º"""
        print("=" * 80)
        print("AGENT PERFORMANCE COMPARISON RESULTS")
        print("=" * 80)
        
        # ãƒ˜ãƒƒãƒ€ãƒ¼
        print(f"{'Agent':<12} {'Avg Reward':<12} {'Max Score':<10} {'Survival':<10} {'Survival Rate':<12}")
        print("-" * 80)
        
        # çµæœã‚’å¹³å‡å ±é…¬ã§ã‚½ãƒ¼ãƒˆ
        sorted_results = sorted(self.results.items(), 
                               key=lambda x: x[1]['avg_reward'], reverse=True)
        
        for rank, (agent_name, results) in enumerate(sorted_results, 1):
            print(f"{rank}. {agent_name:<9} "
                  f"{results['avg_reward']:.2f}Â±{results['std_reward']:.1f}   "
                  f"{results['max_score']:<10.0f} "
                  f"{results['avg_survival']:<10.1f} "
                  f"{results['survival_rate']:.1%}")
        
        print("-" * 80)
        
        # æœ€é«˜æ€§èƒ½ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ
        best_agent = sorted_results[0]
        print(f"ğŸ† Best Agent: {best_agent[0]} (Avg Reward: {best_agent[1]['avg_reward']:.2f})")
    
    def plot_comparison(self, save_path: str = "agent_comparison.png"):
        """æ¯”è¼ƒçµæœã‚’ã‚°ãƒ©ãƒ•ã§å¯è¦–åŒ–"""
        if not self.results:
            print("No results to plot!")
            return
        
        try:
            fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(12, 10))
            
            agents = list(self.results.keys())
            
            # å¹³å‡å ±é…¬æ¯”è¼ƒ
            avg_rewards = [self.results[agent]['avg_reward'] for agent in agents]
            std_rewards = [self.results[agent]['std_reward'] for agent in agents]
            
            ax1.bar(agents, avg_rewards, yerr=std_rewards, capsize=5, alpha=0.7)
            ax1.set_title('Average Reward Comparison')
            ax1.set_ylabel('Reward')
            ax1.tick_params(axis='x', rotation=45)
            
            # æœ€å¤§ã‚¹ã‚³ã‚¢æ¯”è¼ƒ
            max_scores = [self.results[agent]['max_score'] for agent in agents]
            ax2.bar(agents, max_scores, alpha=0.7, color='orange')
            ax2.set_title('Maximum Score Comparison')
            ax2.set_ylabel('Score')
            ax2.tick_params(axis='x', rotation=45)
            
            # ç”Ÿå­˜æ™‚é–“æ¯”è¼ƒ
            survival_times = [self.results[agent]['avg_survival'] for agent in agents]
            ax3.bar(agents, survival_times, alpha=0.7, color='green')
            ax3.set_title('Average Survival Time')
            ax3.set_ylabel('Steps')
            ax3.tick_params(axis='x', rotation=45)
            
            # ç”Ÿå­˜ç‡æ¯”è¼ƒ
            survival_rates = [self.results[agent]['survival_rate'] * 100 for agent in agents]
            ax4.bar(agents, survival_rates, alpha=0.7, color='red')
            ax4.set_title('Survival Rate')
            ax4.set_ylabel('Percentage (%)')
            ax4.tick_params(axis='x', rotation=45)
            
            plt.tight_layout()
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"Comparison plot saved to: {save_path}")
            
            plt.show()
            
        except Exception as e:
            print(f"Plotting error: {e}")
    
    def save_results(self, filepath: str = "comparison_results.txt"):
        """çµæœã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write("Agent Performance Comparison Results\n")
            f.write("=" * 50 + "\n\n")
            
            for agent_name, results in self.results.items():
                f.write(f"{agent_name}:\n")
                for key, value in results.items():
                    if isinstance(value, float):
                        f.write(f"  {key}: {value:.4f}\n")
                    else:
                        f.write(f"  {key}: {value}\n")
                f.write("\n")
        
        print(f"Results saved to: {filepath}")


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    print("Mega Wing ML - Agent Comparison Demo")
    print("=" * 60)
    
    try:
        # æ¯”è¼ƒå®Ÿè¡Œ
        comparison = AgentComparison(evaluation_episodes=10, max_steps=2000)
        results = comparison.run_comparison()
        
        # çµæœè¡¨ç¤º
        comparison.print_comparison_table()
        
        # çµæœä¿å­˜
        comparison.save_results("ml/agent_comparison_results.txt")
        
        # ã‚°ãƒ©ãƒ•ä½œæˆ
        comparison.plot_comparison("ml/agent_comparison.png")
        
        print("\næ¯”è¼ƒãƒ‡ãƒ¢å®Œäº†ï¼")
        print("è©³ç´°çµæœ: ml/agent_comparison_results.txt")
        print("ã‚°ãƒ©ãƒ•: ml/agent_comparison.png")
        
    except KeyboardInterrupt:
        print("\næ¯”è¼ƒãƒ‡ãƒ¢ã‚’ä¸­æ–­ã—ã¾ã—ãŸã€‚")
    except Exception as e:
        print(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()