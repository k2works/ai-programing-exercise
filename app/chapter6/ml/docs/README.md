# Mega Wing ML Integration

シューティングゲーム「Mega Wing」の機械学習統合プロジェクト

## 実装完了機能

### ✅ Phase 1: 基礎実装 (完了)
### ✅ Phase 2: DQN/PPOエージェント訓練基盤 (完了)

#### GameStateExtractor
- **ベクトル状態抽出**: 20次元の正規化されたゲーム状態
  - プレイヤー情報: 位置、射撃可能状態、タイマー
  - ゲーム情報: スコア、レベル
  - 敵情報: 最も近い2体の敵の位置・タイプ・HP
  - 弾丸情報: 最も危険な3発の敵弾の位置

- **グリッド状態抽出**: (5チャンネル, 32高さ, 24幅)
  - チャンネル0: プレイヤー位置
  - チャンネル1-3: 敵タイプ別位置とHP
  - チャンネル4: 敵弾位置

#### MegaWingEnv (OpenAI Gym互換)
- **アクション空間**: 6種類 [何もしない, 左, 右, 上, 下, 射撃]
- **観測空間**: ベクトル(20次元)またはグリッド(5×32×24)
- **報酬設計**:
  - 敵撃破: +100
  - 生存時間: +1/frame
  - プレイヤー被弾: -50
  - ゲームオーバー: -200
  - レベルアップ: +500
  - 弾丸発射: -0.1 (無駄撃ちペナルティ)

#### テスト・デモ
- **包括的テストスイート**: 全機能の動作確認
- **ランダムエージェント**: ベースライン性能測定
- **ルールベースエージェント**: 簡易戦略デモ

#### DQN/PPOエージェント訓練基盤
- **DQN (Deep Q-Network)**: 離散アクション空間向け価値ベース学習
  - Experience Replay: 100,000サンプルのバッファ
  - Target Network: 1,000ステップ間隔で更新
  - ε-greedy探索: 1.0 → 0.02まで線形減衰

- **PPO (Proximal Policy Optimization)**: 安定した方策勾配学習
  - Actor-Critic: 方策と価値関数の同時学習
  - Clipping: 0.2の範囲でポリシー更新を制限
  - GAE (Generalized Advantage Estimation): λ=0.95

#### 学習結果・ベンチマーク (5,000ステップ短時間学習)
- **ランダムエージェント**: 平均報酬 2,004
- **ルールベースエージェント**: 平均報酬 2,244  
- **DQNエージェント**: 平均報酬 868 (学習中)
- **PPOエージェント**: 平均報酬 960 (学習中)

*注: 短時間学習のため、より長い訓練でパフォーマンス向上が期待される*

#### 訓練スクリプト
- **DQN訓練**: `train_dqn_agent.py` - カスタマイズ可能なハイパーパラメータ
- **PPO訓練**: `train_ppo_agent.py` - 並列環境対応
- **クイックテスト**: `quick_training_test.py` - 5,000ステップ動作確認

### ✅ Phase 3: 高度なAI機能 (完了)

#### SmartEnemy (学習型敵AI)
- **個別ニューラルネットワーク**: 各敵が独自の小規模ニューラルネットワーク（状態次元6、行動次元4）を持つ
- **リアルタイム学習**: Policy Gradientアルゴリズムによるオンライン学習
- **適応行動**: プレイヤーの行動パターンに応じて戦略を調整
- **経験蓄積**: 状態、行動、報酬、次状態の経験から学習を実行
- **実装ファイル**: `lib/smart_enemy.py`, `test_smart_enemy.py`

#### PlayerAssistAI (プレイヤー支援AI)
- **AutoAimSystem (自動照準システム)**:
  - 最適ターゲット選択（距離、HP、角度による評価）
  - 弾道予測による照準補正
  - プレイヤー入力との調和的統合

- **AvoidanceSystem (回避支援システム)**:
  - 敵弾の軌道予測（30ステップ先まで）
  - 衝突リスク評価と安全方向検索
  - プレイヤー移動入力への補正提案

- **統合支援機能**:
  - 移動支援：回避推奨と自動照準の統合
  - 射撃推奨：戦術的状況判断による射撃タイミング提案
  - リアルタイム統計：支援効果の可視化

- **実装ファイル**: `lib/player_assist_ai.py`, `test_player_assist_ai.py`

#### 包括的テストスイート
- **SmartEnemy テスト**: 17個のテストケースで学習機能を完全検証
- **PlayerAssistAI テスト**: 17個のテストケースで支援機能を完全検証
- **統合テスト**: 複雑なゲーム状況での動作確認
- **パフォーマンステスト**: 大量オブジェクトでの処理性能検証

#### クイックデモンストレーション
- **統合動作確認**: 全MLシステムの連携動作テスト
- **実機シナリオ**: 実際のゲーム状況でのAI機能実証
- **性能ベンチマーク**: 処理時間と精度の定量評価

## 使用方法

### 基本テスト実行
```bash
cd app/chapter6
# 基礎機能テスト
uv run python test_ml_integration.py

# 短時間学習テスト (5,000ステップ)
uv run python quick_training_test.py

# 本格的なDQN学習 (50,000ステップ)
uv run python train_dqn_agent.py

# 本格的なPPO学習 (50,000ステップ)
uv run python train_ppo_agent.py
```

### 環境の単体使用
```python
from lib.mega_wing_env import MegaWingEnv

# 環境作成
env = MegaWingEnv(observation_type="vector")

# リセット
obs, info = env.reset()

# ステップ実行
action = 5  # 射撃
obs, reward, done, truncated, info = env.step(action)
```

## 技術仕様

### 依存関係
```toml
gymnasium = "^1.2.0"    # Gym互換環境
numpy = "^2.3.2"        # 数値計算
torch = "^2.7.1"        # PyTorch
stable-baselines3 = "^2.7.0"  # 強化学習
opencv-python = "^4.11.0"     # 画像処理
matplotlib = "^3.10.5"  # 可視化
```

### パフォーマンス
- **環境リセット**: ~1ms
- **ステップ実行**: ~0.5ms
- **状態抽出**: ~0.1ms
- **メモリ使用量**: ~10MB

### 観測空間詳細

#### ベクトル観測 (20次元)
1. `player_x`: プレイヤーX座標 (0-1正規化)
2. `player_y`: プレイヤーY座標 (0-1正規化)  
3. `can_shoot`: 射撃可能フラグ (0または1)
4. `shot_timer`: 射撃タイマー (0-1正規化)
5. `score`: スコア (0-1正規化)
6. `level`: レベル (0-1正規化)
7-10. `enemy1_x,y,type,hp`: 最も近い敵の情報
11-14. `enemy2_x,y,type,hp`: 2番目に近い敵の情報
15-20. `bullet1-3_x,y`: 最も危険な敵弾3発の位置

## 学習環境の特徴

### 報酬シェイピング
- **密な報酬**: フレーム毎の生存報酬でエージェント学習を促進
- **スパースボーナス**: 敵撃破やレベルアップでの大きな報酬
- **負のフィードバック**: 被弾や無駄撃ちによるペナルティ

### 状態表現
- **正規化**: 全ての値を0-1範囲に正規化し学習安定性を向上
- **重要情報**: プレイヤーに最も関連する敵と弾丸のみを抽出
- **冗長性除去**: 不要な情報を除いてネットワークサイズを最適化

### 拡張性
- **モジュラー設計**: 新しい報酬やゲーム要素を簡単に追加可能
- **多様な観測**: ベクトル・グリッド両方の観測形式をサポート
- **デバッグ機能**: 詳細なinfo辞書でエージェントの動作分析が可能

---

## 開発ログ

- ✅ 2025-08-06: 基礎実装完了、全テスト通過
- ✅ 2025-08-06: DQN/PPO訓練基盤完了、短時間学習テスト成功
- 🚧 Next: 敵AI強化、プレイヤー支援AI実装予定

## パフォーマンス分析

### 学習効率
- **DQN**: 2.03秒で5,000ステップ学習、3,217 fps
- **PPO**: 3.30秒で5,000ステップ学習、1,628 fps

### 現在の課題
1. **学習時間不足**: 5,000ステップでは本格的な学習に不十分
2. **報酬設計**: ベースライン以下の性能、報酬関数の調整が必要
3. **探索vs活用**: 短期学習では探索期間が支配的

### 改善の方向性
1. **長期学習**: 50,000-100,000ステップでの本格訓練
2. **報酬シェイピング**: より密な報酬設計での学習促進
3. **カリキュラム学習**: 段階的な難易度調整